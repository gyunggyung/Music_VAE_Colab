{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20253,
     "status": "ok",
     "timestamp": 1672539100068,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "XQZGn9m0guNQ",
    "outputId": "b01cb5fb-897a-490e-af91-bf6aebacbdb7"
   },
   "outputs": [],
   "source": [
    "# 재사용을 위하여, 구글 드라이브에 연결\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115261,
     "status": "ok",
     "timestamp": 1672539215323,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "tlRC1ikBgwAp",
    "outputId": "911e62c0-9456-4351-a322-10fa1ff100f6"
   },
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb#scrollTo=0x8YTRDwv8Gk \n",
    "# 여러가지 잡다한 문제들로, 설치 부분 대부분 그대로 사용. tf1을 사용해서 문제가 더 많은 것으로 예상. magenta==2.1.0 부분만 수정.\n",
    "import glob\n",
    "\n",
    "BASE_DIR = \"gs://download.magenta.tensorflow.org/models/music_vae/colab2\"\n",
    "\n",
    "print('Installing dependencies...')\n",
    "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
    "!pip install -q pyfluidsynth\n",
    "!pip install -qU magenta==2.1.0\n",
    "\n",
    "# Hack to allow python to pick up the newly-installed fluidsynth lib.\n",
    "# This is only needed for the hosted Colab environment.\n",
    "import ctypes.util\n",
    "orig_ctypes_util_find_library = ctypes.util.find_library\n",
    "def proxy_find_library(lib):\n",
    "    if lib == 'fluidsynth':\n",
    "        return 'libfluidsynth.so.1'\n",
    "    else:\n",
    "        return orig_ctypes_util_find_library(lib)\n",
    "ctypes.util.find_library = proxy_find_library\n",
    "\n",
    "\n",
    "print('Importing libraries and defining some helper functions...')\n",
    "from google.colab import files\n",
    "import magenta.music as mm\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae.trained_model import TrainedModel\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Necessary until pyfluidsynth is updated (>1.2.5).\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def play(note_sequence):\n",
    "    mm.play_sequence(note_sequence, synth=mm.fluidsynth)\n",
    "\n",
    "def interpolate(model, start_seq, end_seq, num_steps, max_length=32,\n",
    "                assert_same_length=True, temperature=0.5,\n",
    "                individual_duration=4.0):\n",
    "    \"\"\"Interpolates between a start and end sequence.\"\"\"\n",
    "    note_sequences = model.interpolate(\n",
    "        start_seq, end_seq,num_steps=num_steps, length=max_length,\n",
    "        temperature=temperature,\n",
    "        assert_same_length=assert_same_length)\n",
    "\n",
    "    print('Start Seq Reconstruction')\n",
    "    play(note_sequences[0])\n",
    "    print('End Seq Reconstruction')\n",
    "    play(note_sequences[-1])\n",
    "    print('Mean Sequence')\n",
    "    play(note_sequences[num_steps // 2])\n",
    "    print('Start -> End Interpolation')\n",
    "    interp_seq = mm.sequences_lib.concatenate_sequences(\n",
    "        note_sequences, [individual_duration] * len(note_sequences))\n",
    "    play(interp_seq)\n",
    "    mm.plot_sequence(interp_seq)\n",
    "    return interp_seq if num_steps > 3 else note_sequences[num_steps // 2]\n",
    "\n",
    "def download(note_sequence, filename):\n",
    "    mm.sequence_proto_to_midi_file(note_sequence, filename)\n",
    "    files.download(filename)\n",
    "\n",
    "print('Done')\n",
    "\n",
    "# 추가 import\n",
    "from copy import deepcopy\n",
    "import pathlib\n",
    "import zipfile\n",
    "\n",
    "from magenta.scripts.convert_dir_to_note_sequences import convert_directory\n",
    "\n",
    "import note_seq\n",
    "\n",
    "from magenta.models.music_vae import configs\n",
    "from magenta.models.music_vae import data\n",
    "import tf_slim\n",
    "\n",
    "import collections\n",
    "\n",
    "from magenta.common import merge_hparams\n",
    "from magenta.contrib import training as contrib_training\n",
    "from magenta.models.music_vae import data_hierarchical\n",
    "from magenta.models.music_vae import lstm_models\n",
    "from magenta.models.music_vae.base_model import MusicVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1672539215699,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "-PMt__XblOnu"
   },
   "outputs": [],
   "source": [
    "def make_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# 경로 설정 및 필요한 폴더 생성\n",
    "path = \"/content/gdrive/MyDrive/과제/포자랩스/\"\n",
    "\n",
    "make_directory(path + \"data_dir\")\n",
    "make_directory(path + 'train')\n",
    "make_directory(path + 'gen_midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1672539215700,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "Rtj1jucGh7bw"
   },
   "outputs": [],
   "source": [
    "def make_tfrecord(path):\n",
    "    # 파일 다운로드 + 압축 해제\n",
    "    root_dir= path + 'groove'\n",
    "    output_file = path + 'data_dir/music.tfrecord'\n",
    "    save_groove_dataset_path = path + 'data_dir/data.zip'\n",
    "    groove_midionly_url = \"https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\"\n",
    "\n",
    "    tf.keras.utils.get_file(fname=save_groove_dataset_path, origin=groove_midionly_url, extract=True)\n",
    "    zipfile.ZipFile(save_groove_dataset_path).extractall(path)\n",
    "\n",
    "    # 아래 함수를 이용해서 데이터 전처리\n",
    "\n",
    "    # https://github.com/magenta/magenta/blob/77ed668af96edea7c993d38973b9da342bd31e82/magenta/scripts/convert_dir_to_note_sequences.py\n",
    "    # def convert_directory(root_dir, output_file, recursive=False):\n",
    "    #   \"\"\"Converts files to NoteSequences and writes to `output_file`.\n",
    "    #   Input files found in `root_dir` are converted to NoteSequence protos with the\n",
    "    #   basename of `root_dir` as the collection_name, and the relative path to the\n",
    "    #   file from `root_dir` as the filename. If `recursive` is true, recursively\n",
    "    #   converts any subdirectories of the specified directory.\n",
    "    #   Args:\n",
    "    #     root_dir: A string specifying a root directory.\n",
    "    #     output_file: Path to TFRecord file to write results to.\n",
    "    #     recursive: A boolean specifying whether or not recursively convert files\n",
    "    #         contained in subdirectories of the specified directory.\n",
    "    #   \"\"\"\n",
    "    #   with tf.io.TFRecordWriter(output_file) as writer:\n",
    "    #     convert_files(root_dir, '', writer, recursive)\n",
    "\n",
    "    convert_directory(root_dir, output_file, recursive=True)\n",
    "    \n",
    "#   def convert_files(root_dir, sub_dir, writer, recursive=False):\n",
    "#       \"\"\"Converts files.\n",
    "#       Args:\n",
    "#         root_dir: A string specifying a root directory.\n",
    "#         sub_dir: A string specifying a path to a directory under `root_dir` in which\n",
    "#             to convert contents.\n",
    "#         writer: A TFRecord writer\n",
    "#         recursive: A boolean specifying whether or not recursively convert files\n",
    "#             contained in subdirectories of the specified directory.\n",
    "#       Returns:\n",
    "#         A map from the resulting Futures to the file paths being converted.\n",
    "#       \"\"\"\n",
    "#       dir_to_convert = os.path.join(root_dir, sub_dir)\n",
    "#       tf.logging.info(\"Converting files in '%s'.\", dir_to_convert)\n",
    "#       files_in_dir = tf.gfile.ListDirectory(os.path.join(dir_to_convert))\n",
    "#       recurse_sub_dirs = []\n",
    "#       written_count = 0\n",
    "#       for file_in_dir in files_in_dir:\n",
    "#         tf.logging.log_every_n(tf.logging.INFO, '%d files converted.',\n",
    "#                                1000, written_count)\n",
    "#         full_file_path = os.path.join(dir_to_convert, file_in_dir)\n",
    "#         if (full_file_path.lower().endswith('.mid') or\n",
    "#             full_file_path.lower().endswith('.midi')):\n",
    "#           try:\n",
    "#             sequence = convert_midi(root_dir, sub_dir, full_file_path)\n",
    "#           except Exception as exc:  # pylint: disable=broad-except\n",
    "#             tf.logging.fatal('%r generated an exception: %s', full_file_path, exc)\n",
    "#             continue\n",
    "#           if sequence:\n",
    "#             writer.write(sequence.SerializeToString())\n",
    "#         elif (full_file_path.lower().endswith('.xml') or\n",
    "#               full_file_path.lower().endswith('.mxl')):\n",
    "#           try:\n",
    "#             sequence = convert_musicxml(root_dir, sub_dir, full_file_path)\n",
    "#           except Exception as exc:  # pylint: disable=broad-except\n",
    "#             tf.logging.fatal('%r generated an exception: %s', full_file_path, exc)\n",
    "#             continue\n",
    "#           if sequence:\n",
    "#             writer.write(sequence.SerializeToString())\n",
    "#         elif full_file_path.lower().endswith('.abc'):\n",
    "#           try:\n",
    "#             sequences = convert_abc(root_dir, sub_dir, full_file_path)\n",
    "#           except Exception as exc:  # pylint: disable=broad-except\n",
    "#             tf.logging.fatal('%r generated an exception: %s', full_file_path, exc)\n",
    "#             continue\n",
    "#           if sequences:\n",
    "#             for sequence in sequences:\n",
    "#               writer.write(sequence.SerializeToString())\n",
    "#         else:\n",
    "#           if recursive and tf.gfile.IsDirectory(full_file_path):\n",
    "#             recurse_sub_dirs.append(os.path.join(sub_dir, file_in_dir))\n",
    "#           else:\n",
    "#             tf.logging.warning(\n",
    "#                 'Unable to find a converter for file %s', full_file_path)\n",
    "\n",
    "#       for recurse_sub_dir in recurse_sub_dirs:\n",
    "#         convert_files(root_dir, recurse_sub_dir, writer, recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1672539674059,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "dk-LntS_Gk0M"
   },
   "outputs": [],
   "source": [
    "def get_config(batch_size = 128, num_bar = 4):\n",
    "    # https://github.com/magenta/magenta/blob/main/magenta/models/music_vae/configs.py\n",
    "    # 에서 필요한 부분 수정 논문의 구현과 비슷하게, 드럼만 뽑아내는 coonfig 작성\n",
    "\n",
    "    \"\"\"Configurations for MusicVAE models.\"\"\"\n",
    "\n",
    "\n",
    "    HParams = contrib_training.HParams\n",
    "\n",
    "\n",
    "    class Config(collections.namedtuple(\n",
    "        'Config',\n",
    "        ['model', 'hparams', 'note_sequence_augmenter', 'data_converter',\n",
    "            'train_examples_path', 'eval_examples_path', 'tfds_name'])):\n",
    "\n",
    "        def values(self):\n",
    "            return self._asdict()\n",
    "\n",
    "    Config.__new__.__defaults__ = (None,) * len(Config._fields)\n",
    "\n",
    "\n",
    "    def update_config(config, update_dict):\n",
    "        config_dict = config.values()\n",
    "        config_dict.update(update_dict)\n",
    "        return Config(**config_dict)\n",
    "\n",
    "    # https://github.com/magenta/magenta/issues/1549 참고\n",
    "    CONFIG_MAP = Config(\n",
    "        model=MusicVAE(\n",
    "            lstm_models.BidirectionalLstmEncoder(), \n",
    "            lstm_models.HierarchicalLstmDecoder(\n",
    "                    lstm_models.SplitMultiOutLstmDecoder(\n",
    "                        core_decoders=[\n",
    "                            lstm_models.CategoricalLstmDecoder()],\n",
    "                        output_depths=[\n",
    "                            512,  # drums\n",
    "                    ]),\n",
    "                    level_lengths=[16, num_bar],\n",
    "                    disable_autoregression=True)), #Hierarchical Decoder\n",
    "        hparams=merge_hparams(\n",
    "            lstm_models.get_default_hparams(),\n",
    "            HParams(\n",
    "                batch_size=batch_size, # 512\n",
    "                max_seq_len=16*num_bar,\n",
    "                z_size=512, # 512\n",
    "                enc_rnn_size=[2048, 2048], # 2048, 2048\n",
    "                dec_rnn_size=[1024, 1024], # 1024, 1024\n",
    "                free_bits=256,\n",
    "                max_beta=0.2,\n",
    "            )),\n",
    "        note_sequence_augmenter=None,\n",
    "        data_converter=data.DrumsConverter(\n",
    "            max_bars=num_bar,  # Truncate long drum sequences before slicing.\n",
    "            slice_bars=num_bar,\n",
    "            steps_per_quarter=num_bar,\n",
    "            roll_input=True),\n",
    "        train_examples_path=path + 'data_dir/music.tfrecord',\n",
    "        eval_examples_path=None,\n",
    "        tfds_name='groove-4bar-midionly',\n",
    "    )\n",
    "\n",
    "    return CONFIG_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1672539215701,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "DOtadHeYg7rG"
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, config_map, path):\n",
    "        self.config_map = config_map\n",
    "        self.path = path\n",
    "\n",
    "    # https://github.com/magenta/magenta/blob/77ed668af96edea7c993d38973b9da342bd31e82/magenta/models/music_vae/music_vae_train.py\n",
    "    # 에서 필요한 부분 뽑아서 학습 코드 작성\n",
    "\n",
    "    # Should not be called from within the graph to avoid redundant summaries.\n",
    "    @staticmethod\n",
    "    def _trial_summary(hparams, examples_path, output_dir):\n",
    "        \"\"\"Writes a tensorboard text summary of the trial.\"\"\"\n",
    "\n",
    "        examples_path_summary = tf.summary.text(\n",
    "            'examples_path', tf.constant(examples_path, name='examples_path'),\n",
    "            collections=[])\n",
    "\n",
    "        hparams_dict = hparams.values()\n",
    "\n",
    "        # Create a markdown table from hparams.\n",
    "        header = '| Key | Value |\\n| :--- | :--- |\\n'\n",
    "        keys = sorted(hparams_dict.keys())\n",
    "        lines = ['| %s | %s |' % (key, str(hparams_dict[key])) for key in keys]\n",
    "        hparams_table = header + '\\n'.join(lines) + '\\n'\n",
    "\n",
    "        hparam_summary = tf.summary.text(\n",
    "            'hparams', tf.constant(hparams_table, name='hparams'), collections=[])\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            writer = tf.summary.FileWriter(output_dir, graph=sess.graph)\n",
    "            writer.add_summary(examples_path_summary.eval())\n",
    "            writer.add_summary(hparam_summary.eval())\n",
    "            writer.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_input_tensors(dataset, config):\n",
    "        \"\"\"Get input tensors from dataset.\"\"\"\n",
    "        batch_size = config.hparams.batch_size\n",
    "        iterator = tf.data.make_one_shot_iterator(dataset)\n",
    "        (input_sequence, output_sequence, control_sequence,\n",
    "        sequence_length) = iterator.get_next()\n",
    "        input_sequence.set_shape(\n",
    "            [batch_size, None, config.data_converter.input_depth])\n",
    "        output_sequence.set_shape(\n",
    "            [batch_size, None, config.data_converter.output_depth])\n",
    "        if not config.data_converter.control_depth:\n",
    "            control_sequence = None\n",
    "        else:\n",
    "            control_sequence.set_shape(\n",
    "                [batch_size, None, config.data_converter.control_depth])\n",
    "        sequence_length.set_shape([batch_size] + sequence_length.shape[1:].as_list())\n",
    "\n",
    "        return {\n",
    "            'input_sequence': input_sequence,\n",
    "            'output_sequence': output_sequence,\n",
    "            'control_sequence': control_sequence,\n",
    "            'sequence_length': sequence_length\n",
    "        }\n",
    "\n",
    "    def train(self, train_dir,\n",
    "            config,\n",
    "            dataset_fn,\n",
    "            checkpoints_to_keep=5,\n",
    "            keep_checkpoint_every_n_hours=1,\n",
    "            num_steps=None,\n",
    "            master='',\n",
    "            num_sync_workers=0,\n",
    "            num_ps_tasks=0,\n",
    "            task=0):\n",
    "        \"\"\"Train loop.\"\"\"\n",
    "        tf.gfile.MakeDirs(train_dir)\n",
    "        is_chief = (task == 0)\n",
    "        if is_chief:\n",
    "            self._trial_summary(\n",
    "                config.hparams, config.train_examples_path or config.tfds_name,\n",
    "                train_dir)\n",
    "        with tf.Graph().as_default():\n",
    "            with tf.device(tf.train.replica_device_setter(\n",
    "                num_ps_tasks, merge_devices=True)):\n",
    "\n",
    "                model = config.model\n",
    "                model.build(config.hparams,\n",
    "                            config.data_converter.output_depth,\n",
    "                            is_training=True)\n",
    "\n",
    "                optimizer = model.train(**self._get_input_tensors(dataset_fn(), config))\n",
    "\n",
    "                hooks = []\n",
    "                if num_sync_workers:\n",
    "                    optimizer = tf.train.SyncReplicasOptimizer(\n",
    "                        optimizer,\n",
    "                        num_sync_workers)\n",
    "                    hooks.append(optimizer.make_session_run_hook(is_chief))\n",
    "\n",
    "                grads, var_list = list(zip(*optimizer.compute_gradients(model.loss)))\n",
    "                global_norm = tf.global_norm(grads)\n",
    "                tf.summary.scalar('global_norm', global_norm)\n",
    "\n",
    "                if config.hparams.clip_mode == 'value':\n",
    "                    g = config.hparams.grad_clip\n",
    "                    clipped_grads = [tf.clip_by_value(grad, -g, g) for grad in grads]\n",
    "                elif config.hparams.clip_mode == 'global_norm':\n",
    "                    clipped_grads = tf.cond(\n",
    "                        global_norm < config.hparams.grad_norm_clip_to_zero,\n",
    "                        lambda: tf.clip_by_global_norm(  # pylint:disable=g-long-lambda\n",
    "                            grads, config.hparams.grad_clip, use_norm=global_norm)[0],\n",
    "                        lambda: [tf.zeros(tf.shape(g)) for g in grads])\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        'Unknown clip_mode: {}'.format(config.hparams.clip_mode))\n",
    "                train_op = optimizer.apply_gradients(\n",
    "                    list(zip(clipped_grads, var_list)),\n",
    "                    global_step=model.global_step,\n",
    "                    name='train_step')\n",
    "\n",
    "                logging_dict = {'global_step': model.global_step,\n",
    "                                'loss': model.loss}\n",
    "\n",
    "                hooks.append(tf.train.LoggingTensorHook(logging_dict, every_n_iter=100))\n",
    "                if num_steps:\n",
    "                    hooks.append(tf.train.StopAtStepHook(last_step=num_steps))\n",
    "\n",
    "                scaffold = tf.train.Scaffold(\n",
    "                    saver=tf.train.Saver(\n",
    "                        max_to_keep=checkpoints_to_keep,\n",
    "                        keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours))\n",
    "                tf_slim.training.train(\n",
    "                    train_op=train_op,\n",
    "                    logdir=train_dir,\n",
    "                    scaffold=scaffold,\n",
    "                    hooks=hooks,\n",
    "                    save_checkpoint_secs=60,\n",
    "                    master=master,\n",
    "                    is_chief=is_chief)\n",
    "\n",
    "\n",
    "    def run(self, num_steps=100,\n",
    "            tf_file_reader=tf.data.TFRecordDataset,\n",
    "            file_reader=tf.python_io.tf_record_iterator):\n",
    "        \"\"\"Load model params, save config file and start trainer.\n",
    "        Args:\n",
    "        config_map: Dictionary mapping configuration name to Config object.\n",
    "        tf_file_reader: The tf.data.Dataset class to use for reading files.\n",
    "        file_reader: The Python reader to use for reading files.\n",
    "        Raises:\n",
    "        ValueError: if required flags are missing or invalid.\n",
    "        \"\"\"\n",
    "        train_dir = os.path.join(self.path, 'train')\n",
    "\n",
    "        config = self.config_map\n",
    "\n",
    "        def dataset_fn():\n",
    "            return data.get_dataset(\n",
    "                config,\n",
    "                tf_file_reader=tf_file_reader,\n",
    "                is_training=True,\n",
    "                cache_dataset=True)\n",
    "            \n",
    "        self.train(\n",
    "            train_dir,\n",
    "            config=config,\n",
    "            dataset_fn=dataset_fn,\n",
    "            num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1672539215704,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "OpDYrNJDkQ9Y"
   },
   "outputs": [],
   "source": [
    "def music_vae_generate(config_map, path, output_dir, num_steps, temperature = 1.5, num_gens = 5, length_gens = 32, batch_size = 1):\n",
    "    # https://github.com/magenta/magenta/blob/main/magenta/models/music_vae/music_vae_generate.py\n",
    "    # 에서 필요한 부분 뽑아서 작성\n",
    "\n",
    "    model = TrainedModel(\n",
    "        config=config_map, batch_size=batch_size,\n",
    "        checkpoint_dir_or_path=path + '/train')\n",
    "\n",
    "    results = model.sample(\n",
    "        n=num_gens, \n",
    "        length=length_gens, \n",
    "        temperature=temperature)\n",
    "\n",
    "    # https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb#scrollTo=0x8YTRDwv8Gk \n",
    "    # 에서 다운로드 부분 사용\n",
    "    for i, ns in enumerate(results):\n",
    "        download(ns, output_dir + '/%s_sample_step_%d_%d.mid' % (\"groovae_4bar\", num_steps, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 207843,
     "status": "ok",
     "timestamp": 1672539489964,
     "user": {
      "displayName": "x x",
      "userId": "03209786755063406959"
     },
     "user_tz": -540
    },
    "id": "jekcEpRNEfZB",
    "outputId": "bf918778-3aeb-46bd-bc24-c96d77c699da"
   },
   "outputs": [],
   "source": [
    "CONFIG_MAP = get_config(batch_size=512)\n",
    "model_configs = deepcopy(CONFIG_MAP)\n",
    "\n",
    "# 학습 데이터 전처리\n",
    "make_tfrecord(path)\n",
    "\n",
    "# 모델 학습\n",
    "num_steps = 5555\n",
    "trainer = Trainer(CONFIG_MAP, path)\n",
    "trainer.run(num_steps=num_steps)\n",
    "\n",
    "# 음악 생성\n",
    "for temp in np.arange(0.5, 16.0, 1.0):\n",
    "    music_vae_generate(model_configs, path, path + 'gen_midi', num_steps, temperature = temp, length_gens = 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHLPuCgRF7qu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TapjBW2yDjp"
   },
   "source": [
    "# 가벼운 모델 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/magenta/magenta/blob/77ed668af96edea7c993d38973b9da342bd31e82/magenta/models/music_vae/base_model.py\n",
    "https://github.com/magenta/magenta/blob/77ed668af96edea7c993d38973b9da342bd31e82/magenta/models/music_vae/lstm_models.py\n",
    "https://github.com/magenta/magenta/blob/77ed668af96edea7c993d38973b9da342bd31e82/magenta/contrib/rnn.py\n",
    "https://github.com/magenta/magenta/blob/77ed668af96edea7c993d38973b9da342bd31e82/magenta/models/music_vae/lstm_utils.py\n",
    "에서 모델을 자세히 볼 수 있습니다. 아래 내용 중, 혹시 틀린 부분이 있을 경우, 알려주시면 감사하겠습니다! 핵심만 쓰기 위해, 많은 중간과정을 생략하고 있습니다.\n",
    "\n",
    "## ENCODER\n",
    "ENCODER는 논문과 같이 latent distribution parameters(μ and σ)를 만드는데 필요한, BiLSTM의 final state vectors를 얻습니다.\n",
    "\n",
    "## HIERARCHICAL DECODER\n",
    "DECODER는 논문과 같이 HIERARCHICAL DECODER를 사용합니다. 입력 sequence(drum midi)를 4마디마다 subsequences로 분할합니다. subsequences를 unidirectional LSTM에 넣어, 각각의 embedding vectors c(Conductuor)를 듭니다. 이를 이용하여, 생성을 진행합니다. 이러한 방법으로 긴 sequence도 학습 및 생성을 진행할 수 있습니다.\n",
    "\n",
    "## Multi-Stream Modeling\n",
    "우리의 목표는, 4마디에 해당하는 drum 샘플을 뽑아내는 것입니다. 따라서 Multi-Stream Modeling는 사실상 진행하지 않습니다. 다만 구현된 모델에 맞게 학습을 위해, drum distributions만 설정하게 사용했습니다. Multi-Stream Modeling은 다양한 signal로 연주하기 위해서, independent한 3 separate distributions(drum, bass, and melody)을 만듭니다. 이는 별도의 DECODER를 사용하여, 생성을 진행합니다.\n",
    "\n",
    "## ETC\n",
    "이외에 사항들은 VAE 생성 방식과 유사합니다.\n",
    "\n",
    "## Some code\n",
    "\n",
    "```\n",
    "    Encodes input sequences into a precursors for latent code `z`.\n",
    "    Args:\n",
    "       sequence: Batch of sequences to encode.\n",
    "       sequence_length: Length of sequences in input batch.\n",
    "    Returns:\n",
    "       outputs: Raw outputs to parameterize the prior distribution in\n",
    "          MusicVae.encode, sized `[batch_size, N]`.\n",
    "    ....\n",
    "    \n",
    "    last_h_fw = states_fw[-1][-1].h\n",
    "    last_h_bw = states_bw[-1][-1].h\n",
    "\n",
    "    return tf.concat([last_h_fw, last_h_bw], 1)\n",
    "```\n",
    "\n",
    "```\n",
    "    \"\"\"Initializer for HierarchicalLstmDecoder.\n",
    "    Hierarchicaly decodes a sequence across time.\n",
    "    Each sequence is padded per-segment. For example, a sequence with\n",
    "    three segments [1, 2, 3], [4, 5], [6, 7, 8 ,9] and a `max_seq_len` of 12\n",
    "    is represented as `sequence = [1, 2, 3, 0, 4, 5, 0, 0, 6, 7, 8, 9]` with\n",
    "    `sequence_length = [3, 2, 4]`.\n",
    "    `z` initializes the first level LSTM to produce embeddings used to\n",
    "    initialize the states of LSTMs at subsequent levels. The lowest-level\n",
    "    embeddings are then passed to the given `core_decoder` to generate the\n",
    "    final outputs.\n",
    "    This decoder has 3 modes for what is used as the inputs to the LSTMs\n",
    "    (excluding those in the core decoder):\n",
    "      Autoregressive: (default) The inputs to the level `l` decoder are the\n",
    "        final states of the level `l+1` decoder.\n",
    "      Non-autoregressive: (`disable_autoregression=True`) The inputs to the\n",
    "        hierarchical decoders are 0's.\n",
    "      Re-encoder: (`hierarchical_encoder` provided) The inputs to the level `l`\n",
    "        decoder are re-encoded outputs of level `l+1`, using the given encoder's\n",
    "        matching level.\n",
    "    Args:\n",
    "      core_decoder: The BaseDecoder implementation to use at the output level.\n",
    "      level_lengths: A list of the number of outputs of each level of the\n",
    "        hierarchy. The final level is the (padded) maximum length. The product\n",
    "        of the lengths must equal `hparams.max_seq_len`.\n",
    "      disable_autoregression: Whether to disable the autoregression within the\n",
    "        hierarchy. May also be a collection of levels on which to disable.\n",
    "      hierarchical_encoder: (Optional) A HierarchicalLstmEncoder instance to use\n",
    "        for re-encoding the decoder outputs at each level for use as inputs to\n",
    "        the next level up in the hierarchy, instead of the final decoder state.\n",
    "        The encoder level output lengths (except for the final single-output\n",
    "        level) should be the reverse of `level_output_lengths`.\n",
    "    Raises:\n",
    "      ValueError: If `hierarchical_encoder` is given but has incompatible level\n",
    "        lengths.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "## Reference\n",
    "- A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music https://arxiv.org/pdf/1803.05428.pdf\n",
    "- magenta/magenta https://github.com/magenta/magenta\n",
    "- Groove MIDI Dataset https://magenta.tensorflow.org/datasets/groove\n",
    "- magenta-demos Colab https://colab.research.google.com/github/magenta/magenta-demos/blob/master/colab-notebooks/MusicVAE.ipynb\n",
    "- MusicVAE - Training stops at epoch 0 with no output or explanation. #1549  https://github.com/magenta/magenta/issues/1549\n",
    "- tf.keras.utils.get_file https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file\n",
    "- zipfile — ZIP 아카이브 작업 https://docs.python.org/ko/3/library/zipfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
